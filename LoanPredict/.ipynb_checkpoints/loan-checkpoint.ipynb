{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values of predicted data [1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1]\n",
      "accuracy score of predicted values 0.7604166666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rajesh\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('loan.csv')\n",
    "\n",
    "#encode the target variable\n",
    "encode = LabelEncoder()\n",
    "\n",
    "data.Loan_Status = encode.fit_transform(data.Loan_Status)\n",
    "\n",
    "#dropping any of the NULL values\n",
    "data.dropna(how='any',inplace=True)\n",
    "\n",
    "#train-test split \n",
    "\n",
    "train,test = train_test_split(data,test_size=0.2,random_state =0)\n",
    "\n",
    "train_x = train.drop(columns=['Loan_ID','Loan_Status'], axis=1)\n",
    "train_y = train['Loan_Status']\n",
    "\n",
    "test_x = test.drop(columns=['Loan_ID','Loan_Status'], axis=1)\n",
    "test_y = test['Loan_Status']\n",
    "\n",
    "\n",
    "#for this data set, we can't use inverse transform directly becaues the data isn't just numerical. The data has some columns which are categorial as well therefore we must use dummy variables.\n",
    "#this is also known as one-hot encoding\n",
    "#get_dummies converts categorical variables into dummy/indicator variables\n",
    "\n",
    "train_x = pd.get_dummies(train_x)\n",
    "test_x = pd.get_dummies(test_x)\n",
    "\n",
    "\n",
    "#need to ensure the shapes for each dataframe is matching / correct\n",
    "#train_x.shape\n",
    "#test_x.shape\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "predict = model.predict(test_x)\n",
    "\n",
    "\n",
    "print('values of predicted data', predict)\n",
    "print('accuracy score of predicted values', accuracy_score(test_y,predict))\n",
    "\n",
    "\n",
    "# print('predicted values on the test data', predict)\n",
    "# print('accuracy score on the test data:\\n\\n')\n",
    "# print(accuracy_score(test_y,predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
